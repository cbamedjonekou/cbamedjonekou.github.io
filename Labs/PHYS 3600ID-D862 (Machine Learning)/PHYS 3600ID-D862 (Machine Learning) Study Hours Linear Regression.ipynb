{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name: Christ-Brian Amedjonekou**  \n",
    "**Date: 04/16/2019**  \n",
    "**PHYS 3600ID-D862 (Machine Learning) Study Hours Linear Regression**  \n",
    "**Spring 2019, Section: D862, Code: 37026**  \n",
    "**Instructor: Viviana Acquaviva**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we use the Study Hours Dataset. It's a simple dataset which we'll use to illustrate the Linear Regression | Gradient Descent Models. The Study Hours Dataset contains one feature vector (amount of hours studied) and one target vector (scores on the exam), both of which are continuous, so Regression seems appropriate here. Since the dataset is simple we expect it to be clean, so no preprosessing is necessary. First, we'll use both Tensorflow and Scikit-Learn implementations, then we'll create a Linear Regression (and Gradient Descent) Model from scratch. We'll also evaluate the model's performance, find/tune the hyperparameters, and the provide visualizations of our results.\n",
    "\n",
    "#### Contents\n",
    "\n",
    "1. [Data Exploration](#Data-Exploration)\n",
    "2. [Data Visualization](#Data-Visualization)\n",
    "3. [Linear Regression Theory](#Linear-Regression-Theory)\n",
    "    1. [Normal Equations](#Normal-Equation:)\n",
    "    2. [Cost Function](#Cost-Function:)\n",
    "4. [Linear-Regression-via-Scikit-Learn](#Linear-Regression-via-Scikit-Learn)\n",
    "    1. [Scikit-Learn Implementation](#Scikit-Learn-Implementation:)\n",
    "    2. [Cross Validation](#Cross-Validation:)\n",
    "    3. [Measuring Performance](#Measuring-Performance:)\n",
    "    4. [Visualizations](#Visualizations:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration\n",
    "\n",
    "[Back to Contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We import and label our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DSH_df = pd.read_csv('Datasets/data_students_hours.csv', \n",
    "                     names= ['Feature: Study Time', 'Target: Test Scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Then we use the `.info()` method to get an idea of how many rows and columns (i.e.: instances, features) are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 2 columns):\n",
      "Feature: Study Time    100 non-null float64\n",
      "Target: Test Scores    100 non-null float64\n",
      "dtypes: float64(2)\n",
      "memory usage: 1.6 KB\n"
     ]
    }
   ],
   "source": [
    "DSH_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `.info()` method shows that we have a 100 rows, and 2 columns: one feature, one target. Additionally, all the values are non-null values. Therefore, filling in values/deleting columns (or rows) is unnecessary. Also all the values in the dataframe (minus the labels) are of type `float64` showing that we are dealing with possibly continuous values.  \n",
    "\n",
    "\n",
    "* **Note**: As aforementioned, this dataset is really simple (and small). Regardless, we'll use it for demonstration purposes.\n",
    "\n",
    "\n",
    "* Next, we'll use  the `.describe()` method to get some basic stats for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature: Study Time</th>\n",
       "      <th>Target: Test Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>48.958341</td>\n",
       "      <td>72.735051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.746379</td>\n",
       "      <td>16.658187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>25.128485</td>\n",
       "      <td>31.707006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>41.526164</td>\n",
       "      <td>60.788586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>49.634970</td>\n",
       "      <td>72.179542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>56.762631</td>\n",
       "      <td>83.215080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>70.346076</td>\n",
       "      <td>118.591217</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Feature: Study Time  Target: Test Scores\n",
       "count           100.000000           100.000000\n",
       "mean             48.958341            72.735051\n",
       "std               9.746379            16.658187\n",
       "min              25.128485            31.707006\n",
       "25%              41.526164            60.788586\n",
       "50%              49.634970            72.179542\n",
       "75%              56.762631            83.215080\n",
       "max              70.346076           118.591217"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DSH_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The `.describe()` method generates descriptive statistics that summarize the central tendency, dispersion and shape of a dataset's distribution. \n",
    "\n",
    "\n",
    "* Usually when we describe central tendency, we usually are refering to the mean, median, or mode; Central Tendency is being used as a representative of the whole. It's what we expect to see; It's what most likely to occur. For this dataset the following is used to represent central tendency: \n",
    "$$\\space$$\n",
    "$$\\text{Mean: Study Times} = 48.95834, \\text{ Test Scores} = 72.735051$$  \n",
    "$$\\text{Quartiles }(50\\%): \\text{Study Times} = 49.634970, \\text{ Test Scores} = 72.179542$$\n",
    "$\\space$\n",
    "    * The 50% Quartile, represents the median of the data set.\n",
    "    \n",
    "    \n",
    "* Standard deviation, and 25% | 50% | 75% Quartiles can be used to  describe the dispersion of the dataset; Dispersion is the extent to which a distribution is stretched or squeezed. Using the first (25%) and third (75%) quartile we can find the interquartile range (IQR). \n",
    "$$\\space$$\n",
    "$$\\text{Quartiles }(25\\%): \\text{Study Times} = 41.526164, \\text{ Test Scores} = 60.788586$$  \n",
    "$$\\text{Quartiles }(75\\%): \\text{Study Times} = 56.762631, \\text{ Test Scores} = 83.215080$$\n",
    "\n",
    "\n",
    "* The minimum and maximum values can also be used to describe dispersion (i.e.: The difference between the two describes the range)\n",
    "    $$\\space$$\n",
    "    * **Minimum:** ($\\text{Study Times} = 25.128485, \\text{ Test Scores} = 31.707006$)\n",
    "    * **Maximum:** ($\\text{Study Times} = 70.346076, \\text{ Test Scores} = 118.591217$)\n",
    "\n",
    "\n",
    "* Now that we provided some descriptive stats, we'll display the first 5 rows of the dataframe using the `.head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature: Study Time</th>\n",
       "      <th>Target: Test Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32.502345</td>\n",
       "      <td>31.707006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.426804</td>\n",
       "      <td>68.777596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.530358</td>\n",
       "      <td>62.562382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47.475640</td>\n",
       "      <td>71.546632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.813208</td>\n",
       "      <td>87.230925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature: Study Time  Target: Test Scores\n",
       "0            32.502345            31.707006\n",
       "1            53.426804            68.777596\n",
       "2            61.530358            62.562382\n",
       "3            47.475640            71.546632\n",
       "4            59.813208            87.230925"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DSH_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n",
    "\n",
    "[Back to Contents](#Contents)\n",
    "\n",
    "* Since the dataset is pretty simple we'll use a scatter plot, the `.scatter()` method, to visualize the distribution of the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defines the Features and Target\n",
    "features_DSH, Target_DSH = DSH_df['Feature: Study Time'], DSH_df['Target: Test Scores']\n",
    "\n",
    "# Plots the Features vs Target\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(features_DSH, Target_DSH,marker='*',color = 'm')\n",
    "ax.set(xlabel= 'Studying Time (Hours)', ylabel= 'Test Scores', \n",
    "       title= 'Study Hours Dataset');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Just by looking at the Scatter Plot you can see an upward trend/positve correlation between the amount of hours spent studying and the Test Scores; The more you study, the better your test scores. \n",
    "\n",
    "\n",
    "* We could also plot a histogram of the data. We'll use the `.histplot()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "DSH_df.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This histogram shows that the distribution of Study Times and Test Scores are 'normal', and not skewed. What is meant by this is that about 68% of the scores fall within 1 deviation of the mean, about 95% within 2 deviations, and 99.7% within 3 deviations.\n",
    "\n",
    "\n",
    "* One could interpret this as the following: a majority of students spend about 40-60 hrs studying which yields a score range of 56 - 90 points.\n",
    "\n",
    "\n",
    "* Since the dataset is really small, visualizations will be limited. In the next section, we'll describe Linear Regression theoretically, then implement Scikit-Learn and Tensorflow methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression Theory\n",
    "\n",
    "[Back to Contents](#Contents) | [Normal Equations](#Normal-Equation:) | [Cost Function](#Cost-Function:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear Regression is a method that is used to model relationships between a target and its features. You can think of the features as independent variables, and the target values as dependent variables. Linear Regression is commonly used for forecasting and predicting quantitive reponses; It is very successful in that domain. Linear Regression, at its simplest, takes on the following form:\n",
    "$$\\space$$\n",
    "$$\\mathbf{\\hat{y} = \\vec{x_{1}}\\cdot\\hat{\\beta_1} + \\hat{\\beta_0}}$$\n",
    "$$\\space$$  \n",
    "    * '$\\hat{\\beta_m}$' (in this case $m = 0, 1$) represents the weights, or parameters you have to find in order satisfy the linear combination. For linear regression by the Least Squares Method, this is an approximation  \n",
    "    $$\\space$$\n",
    "    * '$\\hat{\\beta_0}$' also represents the intercept/bias\n",
    "    $$\\space$$\n",
    "    * '$\\vec{x_{1}}$' represents the independent variable which is a feature (column vector) of the dataset. $x_{0}$ is a variable for the bias term but $x_{0} = 1$ which is why it seems absent above\n",
    "    $$\\space$$\n",
    "    * '$\\hat{y}$' is the dependent variable, or predicted value of the dataset. It's also a column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If a target depends on multiple features, Multiple-Linear (more than 1 feature variable) Regression can be used. It can be expressed using the following linear combination:  \n",
    "$$\\space$$\n",
    "$$\\mathbf{\\hat{y} = \\vec{x_{1}}\\cdot\\hat{\\beta_1} + \\vec{x_{2}}\\cdot\\hat{\\beta_2} + \\vec{x_{3}}\\cdot\\hat{\\beta_3} + ... + \\vec{x_{m}}\\cdot\\hat{\\beta_m} + \\hat{\\beta_0}}$$\n",
    "$$\\space$$ \n",
    "    * '$m$' represents the number of features\n",
    "    * '$\\hat{\\beta_m}$' (in this case $m = 0,1,2,3,...$) represents the weights, or parameters \n",
    "    * '$\\hat{\\beta_0}$' also represents the intercept/bias\n",
    "    * '$x_{m}$' (where $m = 1,2,3,...$) represents the independent variable which is a feature (column vector) of the dataset. '$x_{m}$' has '$n$' instances, where $n = 1,2,3,...$  \n",
    "    * $x_{0}$ is still present. It equals 1 for all instances, '$n$'\n",
    "    * '$\\hat{y}$' is the dependent variable, or the predicted value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* However, whether it's simple or multiple-linear regression, it can be more convenient to  express the above linear combinations (which are in vector form) in their matrix form: \n",
    "$$\\space$$\n",
    "$$\\hat{y} = X \\cdot \\mathbf{\\hat{\\beta}}$$\n",
    "$$\\space$$\n",
    "    * where '$\\hat{\\beta}$' represents the weights, or parameters you have to find. $\\hat{\\beta}$ is column vector of parameters $\\hat{\\beta_m}$. \n",
    "    $$\\space$$\n",
    "    $$\\hat{\\beta} =\n",
    "    \\left[{\\begin{array}{c}\n",
    "    \\hat{\\beta_0} \\\\\n",
    "    \\hat{\\beta_1} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\hat{\\beta_m} \\\\\n",
    "    \\end{array} } \\right]$$\n",
    "    $$\\space$$\n",
    "    * '$X$' represents the the independent variables, the feature values as a matrix\n",
    "    $$\\space$$\n",
    "    $$X =\n",
    "    \\left[{\\begin{matrix}\n",
    "    1 & x_{1,1} & x_{2,1} & \\cdots & x_{m,1} \\\\\n",
    "    1 & x_{1,2} & x_{2,2} & \\cdots & x_{m,2} \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    1 & x_{1,n} & x_{2,n} & \\cdots & x_{m,n} \\\\\n",
    "    \\end{matrix} } \\right]$$\n",
    "    $$\\space$$\n",
    "    * '$\\hat{y}$' is the dependent variable which is the predicted value, or target vector\n",
    "    $$\\space$$\n",
    "    $$\\hat{y} =\n",
    "    \\left[{\\begin{array}{c}\n",
    "    \\hat{y_1} \\\\\n",
    "    \\hat{y_2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\hat{y_n} \\\\\n",
    "    \\end{array} } \\right]$$\n",
    "    $$\\space$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Matrix Form is more convenient because fitting the model using Linear Regression is commonly done with the Least Squares Method (and Normal Equations). This is regardless of the method of implemention, whether it's our own method, Scikit-Learn, or Tensorflow.\n",
    "    \n",
    "##### Normal Equation:\n",
    "\n",
    "[Back to Contents](#Contents) | [Linear Regression Theory](#Linear-Regression-Theory) | [Cost Function](#Cost-Function:) \n",
    "\n",
    "\n",
    "* A Normal Equation, like the one shown below, is an equation that describes the orthogonal relationship between the residual vector $\\big(\\epsilon = y - X \\cdot \\hat{\\beta}\\big)$ and the plane $\\big\\{X \\cdot \\vec{\\beta} \\space | \\space \\beta \\in R^n \\big\\}$ where '$\\vec{\\beta}$' is the ***exact*** parameter vector needed to solve $y = X \\cdot \\vec{\\beta}$. $\\space \\vec{\\beta} =\n",
    "\\left[{\\begin{array}{c}\n",
    "\\beta_0 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_m \\\\\n",
    "\\end{array} } \\right]$. \n",
    "\n",
    "\n",
    "* However, since we have a dataset with more instances '$m$' than features '$n$' to describe them, the result is an inconsistent system. Inconsistent systems indicate that the best we can do are approximations, meaning '$\\hat{y}$' and '$\\mathbf{\\hat{\\beta}}$': \n",
    "\n",
    "> **Normal equations for least squares**\n",
    "> $$X^T \\hat{y} = X^T X \\cdot \\mathbf{\\hat{\\beta}}$$\n",
    "\n",
    "  \n",
    "* We want the best approximations, of course, and the way to do that is by minimizing a cost function.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cost Function:\n",
    "\n",
    "[Back to Contents](#Contents) | [Linear Regression Theory](#Linear-Regression-Theory) | [Normal Equations](#Normal-Equation:) \n",
    "\n",
    "* The cost function that we want minimize expresses the size of the residual vector. In other words, minimizing the cost function minimizes the size of the residual vector.\n",
    "\n",
    "\n",
    "* There are many cost functions that we could use to describe the size of the residual vector, $\\epsilon$. However minimizing one, minimizes the others as they are closely related. Some cost functions include:\n",
    "\n",
    "> ***Types of Cost Functions:***  \n",
    "> * **2-Norm:** $||\\epsilon||_{2} = \\sqrt{\\sum_{j = 1}^{m}\\epsilon_m^2}$  \n",
    "> * **Squared Error (SE):** $\\big(||\\epsilon||_{2} \\big)^2 = \\sum_{j = 1}^{m}\\epsilon_m^2$  \n",
    "> $\\space$  \n",
    "> * **Mean Squared Error (MSE):** $\\frac{\\big(||\\epsilon||_{2} \\big)^2}{m} = \\frac{1}{m}\\sum_{j = 1}^{m}\\epsilon_m^2$ \n",
    "> $$\\space$$  \n",
    "> * **Root Mean Squared Error (RMSE):** $\\frac{||\\epsilon||_{2}}{\\sqrt{m}} = \\sqrt{\\frac{1}{m}\\sum_{j = 1}^{m}\\epsilon_m^2}$    \n",
    "\n",
    "\n",
    "* To minimize the cost function, we have to find the solution to the Normal Equation. In other words, we have to find the value of the approximated vector $\\mathbf{\\hat{\\beta}}$. We rewrite the the Normal Equation in terms of $\\mathbf{\\hat{\\beta}}$ to get the following:\n",
    "\n",
    "> ***Solution to the Normal Equation:***\n",
    "> $$\\mathbf{\\hat{\\beta}} = \\big(X^T X\\big)^{-1} X^T \\hat{y}$$\n",
    "\n",
    "\n",
    "* In the following sections, we'll provide a demonstration via 3 implementations: Scikit-Learn, TensorFlow, and from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression via Scikit-Learn \n",
    "\n",
    "| [Back to Contents](#Contents) | [Scikit-Learn Implementation](#Scikit-Learn-Implementation:) | [Cross Validation](#Cross-Validation:) | [Measuring Performance](#Measuring-Performance:) | [Visualizations](#Visualizations)\n",
    "\n",
    "* First, we'll implement the Linear Regression Model using Scikit-Learn. \n",
    "* Then, we'll assess how the results of our model will generalize to an independent data set using Cross Validation. \n",
    "* We'll also examine the model performance by looking at the $r^2$ scores\n",
    "* Finally, we'll plot a visualization of the model against the data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scikit-Learn Implementation:\n",
    "\n",
    "| [Back to Contents](#Contents) | [Linear-Regression-via-Scikit-Learn](#Linear-Regression-via-Scikit-Learn) | [Cross Validation](#Cross-Validation:) | [Measuring Performance](#Measuring-Performance:) | [Visualizations](#Visualizations:)\n",
    "\n",
    "\n",
    "* For this implementation, we do several things: create the model, fit the data using the training set, and create predictions.  \n",
    "\n",
    "* Creating a function to find the Residuals is also done for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to find the Residuals \n",
    "def finding_residuals(X, y, beta0= model_sklearn.intercept_, \n",
    "                      beta1= model_sklearn.coef_):\n",
    "    \"\"\"\n",
    "    Finds the residual \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    " \n",
    "    y : array_like, shape (n_samples, n_targets)\n",
    "\n",
    "    beta0 : A float, the approximated intercept. \n",
    "            An element of the least square solution, Column Vector beta\n",
    "            \n",
    "    beta1 : A float, the approximated intercept. \n",
    "            An element of the least square solution, Column Vector beta \n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    output : Array-like, the residuals\n",
    "    \"\"\"\n",
    "    beta = np.array([beta0, beta1])\n",
    "    X = np.hstack((np.ones(X.shape), X))\n",
    "    return y.reshape(10,1) - np.dot(X, beta.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We also compare the predicted scores with actual test scores by finding the residuals. If our model is good, then the length of the residual should not be significantly large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Test Scores: \n",
      "[ 94.99769926  78.62497891  83.61979053  61.47421276  71.30648722\n",
      " 100.44243462  93.98406058  66.20623052  82.82955018  58.07598427] \n",
      "\n",
      "Residual as the Difference between Test Scores & Predicted Scores: \n",
      "[[ 11.1191346 ]\n",
      " [ -7.04322424]\n",
      " [  9.53566041]\n",
      " [ -4.0880885 ]\n",
      " [ -4.02475508]\n",
      " [ -3.81466696]\n",
      " [-24.60715672]\n",
      " [ -7.4813338 ]\n",
      " [  2.62202704]\n",
      " [ -3.29092027]] \n",
      "\n",
      "Residual using (r = y - X*beta ): [[-11.1191346 ]\n",
      " [  7.04322424]\n",
      " [ -9.53566041]\n",
      " [  4.0880885 ]\n",
      " [  4.02475508]\n",
      " [  3.81466696]\n",
      " [ 24.60715672]\n",
      " [  7.4813338 ]\n",
      " [ -2.62202704]\n",
      " [  3.29092027]] \n",
      "\n",
      "R^2 scores: 0.6365227334362467\n"
     ]
    }
   ],
   "source": [
    "# Assigning Linear Regression to variable\n",
    "model_sklearn = LinearRegression()\n",
    "\n",
    "\n",
    "# Create a test set and training set\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_DSH.to_numpy().reshape(-1,1), \n",
    "                                                    Target_DSH.to_numpy(), test_size= 0.10, \n",
    "                                                    random_state= 30)\n",
    "\n",
    "# Fitting the model to the training set\n",
    "model_sklearn.fit(X_train, y_train)\n",
    "\n",
    "# Predictions \n",
    "y_pred = model_sklearn.predict(X_test)\n",
    "printString = 'Residual as the Difference between Test Scores & Predicted Scores: \\n{} \\n'\n",
    "printString_ = 'Residual using (r = y - X*beta ): {} \\n'\n",
    "print('Predicted Test Scores: \\n{} \\n'.format(y_pred))\n",
    "print(printString.format((y_pred - y_test).reshape(10,1)))\n",
    "print(printString_.format(finding_residuals(X_test, y_test)))\n",
    "print('R^2 scores: {}'.format(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We found the residuals with two, but equivalent, ways: \n",
    "    \n",
    "    1. Finding the difference between the true test scores and predicted scores which I find to be more intuitive. It answers the question on how close are our predictions to our real scores.\n",
    "    2. Using $\\epsilon = y - X \\cdot \\hat{\\beta}$ to find the length of the residuals.\n",
    "    \n",
    "\n",
    "* This shows pretty reasonable results since only one prediction had a large difference between predicted and real scores. \n",
    "\n",
    "\n",
    "* For a better description of performance, we'll look at the $r^2$ scores. They are printed here but we'll reserve to conversation for *Measuring Performance* section. For now, we'll continue on to *Cross Validation*.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation: \n",
    "\n",
    "| [Back to Contents](#Contents) | [Scikit-Learn Implementation](#Scikit-Learn-Implementation:) | [Linear-Regression-via-Scikit-Learn](#Linear-Regression-via-Scikit-Learn) | [Measuring Performance](#Measuring-Performance:) | [Visualizations](#Visualizations:)\n",
    "\n",
    "* We use k-fold cross validation here to test whether our model performs roughly the same across all train/test splits. By using cross validation, we avoid these potential problems:\n",
    "     \n",
    "    1. We avoid the lack of data problem itself.\n",
    "> In *k*-fold cross validation ($k = \\text{the number of splits}$), we're allowed to select the number of splits we like to use to train/test the model. Each splits participates in the training and testing phase of the model. Additionally, there is no leakage of data across splits as   \n",
    "    2. We avoid the problem of overfitting due to a lack of samples (small dataset). \n",
    "> Overfitting, which results in great training scores but poor test scores, can indictate several things: high model complexity, a lack of data, or high volume of noise which prevents the extraction of meaningful patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-fold cross validation predictions\n",
    "predict_Linear = cross_val_predict(linearModel, features_DSH, Target_DSH, cv= 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Measuring Performance: \n",
    "\n",
    "| [Back to Contents](#Contents) | [Scikit-Learn Implementation](#Scikit-Learn-Implementation:) | [Linear-Regression-via-Scikit-Learn](#Linear-Regression-via-Scikit-Learn) | [Cross Validation](#Cross-Validation:) | [Visualizations](#Visualizations:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('$R^2$ scores:',metrics.r2_score(Target_DSH.to_numpy(), predict_Linear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizations:\n",
    "\n",
    "| [Back to Contents](#Contents) | [Linear-Regression-via-Scikit-Learn](#Linear-Regression-via-Scikit-Learn) | [Scikit-Learn Implementation](#Scikit-Learn-Implementation:) | [Measuring Performance](#Measuring-Performance:) | [Cross Validation](#Cross-Validation:) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plotting the points in a scatter plot\n",
    "plt.scatter(features_DSH, labels_DSH,marker='*',color = 'm');\n",
    "\n",
    "# fitting the model \n",
    "linearModel.fit(features_DSH.reshape(-1,1), labels_DSH)\n",
    "plt.plot(features_DSH, linearModel.coef_* features_DSH + linearModel.intercept_, color = 'k')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('F(Features)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. However these can also be calculated by hand using the formulas on the linear regression slides. Code up the formulas below and report your coefficients for slope and intercept.\n",
    "\n",
    "$$S S_{xy} = \\sum x_i y_i - \\frac{(\\sum x_i \\sum y_i)}{n}$$\n",
    "\n",
    "$$S S_{xx} = \\sum x_i^2 - \\frac{(\\sum x_i)^2}{n}$$\n",
    "\n",
    "$$m = \\frac{S S_{xy}}{S S_{xx}}$$\n",
    "\n",
    "$$b = \\bar{y} - m * \\bar{x}$$\n",
    "\n",
    "$$\\text{where } n = \\text{ number of points and the overhead bar represent the mean.}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssXY(x, y):\n",
    "    \"\"\"Calculates the Sum of Squares for 'x' and 'y'.\"\"\"\n",
    "    n = y.size\n",
    "    return sum(x*y) - ((sum(x)*sum(y))/n)\n",
    "\n",
    "    \n",
    "def ssXX(x):\n",
    "    \"\"\"Calculates the Sum of Squares for 'x'.\"\"\"\n",
    "    n = x.size\n",
    "    return sum(x*x) - ((sum(x)*sum(x))/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope = ssXY(features_DSH, labels_DSH)/ssXX(features_DSH)\n",
    "intercept = labels_DSH.mean() - slope * features_DSH.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('The slope is: {1} and the intercept is: {0}'.format(intercept, slope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the points in a scatter plot\n",
    "plt.scatter(features_DSH, labels_DSH,marker='*',color = 'm');\n",
    "\n",
    "print('$R^2$ scores:',metrics.r2_score(labels_DSH, predict_Linear))\n",
    "\n",
    "# fitting the model \n",
    "plt.plot(features_DSH, slope* features_DSH + intercept, color = 'k')\n",
    "plt.title('Linear Regression Model')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('F(Features)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Looking at the data (hours vs scores), do you think it would make sense to fit these data using a logistic regression model? Why or why not?\n",
    "\n",
    "* **Based on the label data below, Logistic Regression would not be ideal at all. Tt would not make sense to fit this data using a logistic regression model. The reason is because we don't have or expect categorical predictions. Specically, I mean that the target vector ```labels_DSH ``` isn't categorical (n true/false, yes/no, up/down, etc). Check ```labels_DSH[:20] ```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_DSH[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra credit: Run the gradient descent notebook and report the coefficients you found here. Calculate the cost function (found in the linear regression notebook) for the slope, intercept values you found with the gradient descent method and the values you found above, using the formulas. Which set of values is best? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
