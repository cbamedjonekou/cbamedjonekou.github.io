{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worksheet 4: Welcome to the world of variable stars. Due March 19 at 2pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original work statement: \n",
    "\n",
    "Please write your name here to indicate that your worksheet is the result of your own work, and you have not copied from sources without citing them (this is plagiarism and is not acceptable). Identical or very similar worksheet will share the credit.\n",
    "\n",
    "#### Your name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data description\n",
    "\n",
    "The data we use for this worksheet are about stars. Our features are the so-called \"colors\", which give an indication of whether a star emits more blue, green, yellow, or red light. We are trying to predict whether a star is a special type of star called a RR-Lyrae variable star. So our target values will be yes/no (1/0 in the language of numpy arrays)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Load the data for features and target from the appropriate files and save them as numpy arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. Answer the following questions:\n",
    "\n",
    "Is this a classification or regression problem?\n",
    "\n",
    "Is this supervised or unsupervised learning?\n",
    "\n",
    "How many instances are in this data set? \n",
    "\n",
    "How many features? What is their range?\n",
    "\n",
    "How many RR Lyrae are in the data set? \n",
    "\n",
    "What would be the accuracy of a classifier that classifies all objects in the data set as non-RR Lyrae?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Build a decision tree classifier for this data set, using an 80/20 train test split like last time. What is the performance on the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now implement the k-fold cross validation algorithm using the following syntax (note that this assumes that your algorithm is called 'model'):\n",
    "\n",
    "http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Note: if you want to also save the predicted labels, you can do\n",
    "    \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "ypred = cross_val_predict(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Report the scores, and calculate the mean and standard deviation of the scores vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. Calculate and plot the confusion matrix for your model (Note: you will need to copy/paste the \"plot_confusion_matrix\" function from one of the previous notebooks, and use the function cross_val_predict to get the predicted labels). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. Based on the confusion matrix, how many true positive, true negative, false positive, false negative instances do you have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Based on the definitions given on the slides, calculate accuracy, precision and recall. <b>Important:</b> You need to use the formula, not the built-in functions, and write the steps!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. Which evaluation metric is used by the cross validation score above? [To answer this question, you might need to check out the description of the function]. Given the distribution of classes in your data set, do you see a possible issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other evaluation metrics can be found in scikit using the code below.\n",
    "\n",
    "More info: http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print metrics.accuracy_score(ytest, ypred)    \n",
    "print metrics.recall_score(ytest, ypred)      \n",
    "print metrics.precision_score(ytest, ypred)   \n",
    "print metrics.f1_score(ytest, ypred)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. What happens to each of the metrics above if you flip y_test and y_pred in the above? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "10\\. To compare performance for now, let's use the F1 score, a weighted average of precision and recall.\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "Find out how to use the f1 score (instead of accuracy) as an optional argument of the cross_val_score function and report the mean and standard deviation of the scores associated to this evaluation metric.\n",
    "\n",
    "Relevant docs: \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. It is now time to look at some diagnostics. Use an 80/20 training split (so each fold is 20% of the data, like in your cross validation). Report the f1 score obtained by your model\n",
    "when applied to the training set (i.e. when you predict the labels of the training set). How does it compare with the cross validation score? Based on this result, do you think your algorithm suffers from high variance or high bias and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can look at learning curves. \n",
    "\n",
    "Give your data set a good shuffle using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, y = shuffle(X, y, random_state=13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build learning curves, we need to check how the performance of our algorithm changes when using an increasing amount of data. We will check the scores that we obtain when using 20, 40, 60, 80 and 100% of our data set.\n",
    "\n",
    "The following lines demonstrate how to do this to create a new, smaller data set that contains the first 20% of your original data set:    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number_samples = X.shape[0] #total size of data set \n",
    "\n",
    "Xlittle, ylittle = X[:int(number_samples*0.2),:], y[:int(number_samples*0.2)] #what does this do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. Calculate the mean and the standard deviation of the 5-fold cross validation score for the data set above (don't forget to use 'f1' as the scoring parameter of the 'cross_val_score' function, otherwise it will default to accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13\\. Now do the same but using 40, 60, 80 and 100% of your data set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14\\. Based on the results you obtained, do you think getting more training data would help? (Justify your answer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Extra Credit </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "15\\. Save the values and plot the f1 score for the 5 models in a plot where \"percentage of the data set used\" (20, 40, ... 100%) is on the x axis and 'F1 score' is on the y axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16\\. Do the same as above, but this time create an error bar plot where the y errors are given by the standard deviation of the scores you obtained in the 5 folds of the cross-validation. Why is the standard deviation information useful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
