{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worksheet 5: Discover the Higgs Boson. Due April 2 at 2pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original work statement: \n",
    "\n",
    "Please write your name here to indicate that your worksheet is the result of your own work, and you have not copied from sources without citing them (this is plagiarism and is not acceptable). Identical or very similar worksheet will share the credit.\n",
    "\n",
    "#### Your name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this worksheet, we provide you with a (simplified) version of the simulated Higgs boson data challenge, run by Kaggle in 2014. The files are called \"Higgs_features.csv\" and \"Higgs_labels.csv\". The labels are 0 and 1, corresponding to \"no Higgs signal\", and \"Higgs signal\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1\\. Read the data into two numpy arrays, one for features and one for labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2\\. How many instances and features are in this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Plot the distribution of each feature for this data set in a histogram (like we did in on of the previous worksheets), in one plot. Add a legend (with labels \"Feature 1\", \"Feature 2\"...) and set the transparency of the histograms (property \"alpha\") to 0.5 for clarity. Hint: make sure you are plotting each column, not each row!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Do you see anything unusual in the distribution of any of the features? What problem could this cause?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5\\. How many positive (Higgs) events does your data set contain? Based on this, is the data set balanced or unbalanced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6\\. On the basis of your answer to 5, which evaluation metric (accuracy, precision, recall) would you like to pick for this data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7\\. Use a decision tree classifier as your model, and do five fold cross validation on your data, <b> using the scoring parameter you chose above</b>. Report the mean and standard deviation of the scores obtained for the five folds.\n",
    "\n",
    "Note: If you simply set the parameter \"cv = 5\" in the cross_val_score function, this will divide the data in five sets using the first 20%, second 20%... etc of your data, which is not great if your data are in a specific order. Make sure you use cv = StratifiedKFold(shuffle=True, n_splits=5).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8\\. We want to now take a look at the confusion matrix for your classifier. When doing cross validation, sklearn offers a function called cross_val_predict that allows you to save the vector with the predicted values when each object is part of the test set. (http://scikit-learn.org/stable/modules/cross_validation.html). The code for doing that follows; of course you will have to change the name of the model (algorithm) and of the arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "predicted = cross_val_predict(model, X, y, cv=StratifiedKFold(shuffle=True, n_splits=5))\n",
    "\n",
    "print(metrics.confusion_matrix(y, predicted)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9\\. Given your confusion matrix, how many true positives / true negatives / false positives / false negatives are there? Do you have many more type I errors (false positives) than type II errors (false negatives)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10\\. Let's now switch to a SVC classifier. Leaving the parameters of the classifier at their default values, run a five-fold cross validation and report the scores just like you did above for the decision tree. (Note: SVMs are slow so this might take some time, a good 5 minutes on my laptop). Which algorithm performs better? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11\\. Report the confusion matrix for the SVC algorithm. Do you notice a change in the distribution of the false positives and false negatives, with respect to the decision tree algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12\\. We can now do the nested cross validation to optimize the parameters of the SVC. Because it would otherwise take a long time, we can select the first 10% of the data set (after shuffling the data set). We did something like this when we were doing learning curves in Worksheet 4 (questions 11 and 12). Select the first 10% of the data set, both for feature and labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13\\. Set up and run a nested cross validation with 5 outer folds and 3 inner folds. Your GridSearch CV will have the following parameters: \n",
    "\n",
    "parameters = {'kernel':['linear','rbf'],'C':[1, 10, 100], 'gamma':[0.01, 0.1, 0.5], 'class_weight':[{1:1},{1:3},{1:5}]}\n",
    "\n",
    "You can use the last example (\"putting them all together\") from the SVMs and Hyperparameter fitting notebook, but remember to change the scoring parameter to match what you did above.\n",
    "\n",
    "Note: this might take time! Set verbose = 2 in the GridSearchCV to follow the progress, and set njobs = 4 or more to speed up the process. If your machine can't handle it, you can reduce the size of the data set to 5% of the original. \n",
    "\n",
    "#### Report the scores and parameters values of the best model.\n",
    "\n",
    "Note: some times that code has given me an issue of creating \"Nan\" in the ylittle_train arrays. If this happens, you can try substituting the lines\n",
    "\n",
    "ylittle_train = ylittle[train_index]\n",
    "ylittle_test = ylittle[test_index]\n",
    "    \n",
    "with \n",
    "\n",
    "ylittle_train = ylittle.iloc[train_index]\n",
    "ylittle_test = ylittle.iloc[test_index]\n",
    "\n",
    "(see https://stackoverflow.com/questions/39376967/nans-suddenly-appearing-for-sklearn-kfolds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15\\. Now that you have optimized the parameters, you can run the 5 fold cross validation on the original data set, basically what you did in 10., but fixing the parameter values at the \"optimal values\" found above (i.e., choose the model that was most often selected as the winner model). What are the mean and standard deviation of the scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16\\. Based on what you found, would you recommend to use a Decision Tree Classifier or a Support Vector Classifier for this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Credit: The object GridSearchCV (probably called clf in your code) that you created in 13. has an attribute \"grid\\_scores\\_\" that allows you to visualize the scores for every combination of parameters (if you are using sklearn version 0.19 and above, the attribute is named cv\\_results\\_). Based on this, which parameter has the highest impact (induces the largest change) on the performance of the SVC? Do you think it might be worth optimizing it further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
