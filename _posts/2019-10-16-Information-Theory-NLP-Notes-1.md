---
layout: post
mathjax: true
title: "A Brief Summary: Pierce's Introduction to Information Theory (Part 1)"
date: 2019-10-16
description: The first of a series of blog posts that attempts to give a concise introduction to Information Theory. This series of posts can act as a supplement to Pierce's "An Introduction to Information Theory-Symbols, Signals and Noise".
tags: natural-language-processing artificial-intelligence information-theory introduction shannon concise wheel-of-fortune
---

### Chapter 1 - The World and Theories [[Pierce](https://archive.org/details/symbolssignalsan002575mbp/page/n27)]:

1. Introduces the concept of a [Theory](https://en.wikipedia.org/wiki/Theory), and discusses the influence of language when forming theories:

    * Words have varying meaning depending on context; many events/ things are usually referred to by one word. Therefore, to develop theory (a general explanation/representation of phenomenon) the words we use/definitions we give tend to be narrow, and specific. We DO NOT consider all varying meanings of a singular word, or all contexts in which the word applies

    * Theories are formed from an explanation of a collection of phenomenon (related portions of our experience).  They include assumptions we make, and the “mathematical working out of the logical consequences which must necessarily follow from those assumptions”. The consequences must match/agree with the complex phenomena if the theory is to be valid

    * The assumptions and ideas of a theory determines its generality. The more general a theory is, the more broadly it can be applied, and the more useful/powerful it is

2. Describes the classification of theories. The scope/range of the theory as well as its place on the mathematical-physical spectrum determine its classification: 

    * Theories can be classified as being very narrow or very general in its scope. Simply stated, there is a limit to where theories can be applied

    * Theories can also be classified as being strongly mathematical or strongly physical

    * A theory is more physical if it “describes completely some range of physical phenomena which in practices is always limited”  
 
    * A theory is more mathematical if it “deals with an idealized class of phenomena (or certain aspects of phenomena” 

3. Introduces Shannon’s Information Theory (Also known as The Mathematical Theory of Communication). Describes historical origins and details/properties of the theory.

    * Grew out of the study of electrical communication. “However, it attacks problems in a very abstract and general way”; Useful analysis of written/spoken language, electrical/mechanical transmission of messages, behavior of machines/maybe people

    * Introduces the bit. It is a unit of measurement; a universal measure of amount of information in terms of choice or uncertainty

    * Alludes to [Entropy (Information Theory)](https://en.wikipedia.org/wiki/Entropy_(information_theory)), describing an objective of Information Theory as specifying or learning the choice between two equally probable alternatives (i.e: messages, or numbers to be transmitted)

    * Describes how to encode (represent) messages of a particular message source efficiently for transmission over a particular medium/channel

    * Describes abstract properties of particular channels/mediums     


4. Describes Information Theory (and mathematical theory in general) as a set of premises, axioms, and finally theorems (via proofs)

    * “The truth of a theorem depends on the validity of the assumption, argument, and/or proof”

    * [Proofs](https://en.wikipedia.org/wiki/Mathematical_proof) are rigorous (in that they’re complete, consistent, thorough, strict, and adhere to a certain standard) 
