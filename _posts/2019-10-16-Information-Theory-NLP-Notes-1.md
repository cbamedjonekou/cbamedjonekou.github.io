---
layout: post
mathjax: true
title: "A Brief Summary: Pierce's Introduction to Information Theory (Part 1)"
date: 2019-10-16
description: The first of a series of blog posts that attempts to give a concise introduction to Information Theory. This series of posts can act as a supplement to Pierce's "An Introduction to Information Theory-Symbols, Signals and Noise".
tags: natural-language-processing artificial-intelligence information-theory introduction shannon concise wheel-of-fortune
---

<h2 id="Top">Introduction</h2>

In this post we look to provide a brief summary for Chapter 1 (<i>"The World and Theories"</i>) of [Pierce's](https://archive.org/details/symbolssignalsan002575mbp/page/n27) <i>"An Introduction to Information Theory: Symbols, Signals, and Noise</i>. Skip to <a href="#tldr">TL;DR</a> for a summary of the summary (that's so [meta](https://www.grammarly.com/blog/meta-meaning/)).

<div id="Table-of-Contents">
    <h2 id="TOC">Table of Contents</h2>
    <ul>
        <li><a href="#tldr">TL;DR</a></li>
        <li><a href="#theory">What's a Theory</a></li>
        <li><a href="#classification-theory">Classification of Theories</a></li>
        <li><a href="#shannon-overview">Brief Overview of Shannon's Theory of Communication</a></li>
        <li><a href="#Conclusion">Conclusion</a></li>
    </ul>
</div>

<h2 id="tldr">TL;DR</h2>

To be written soon

<h2 id="theory">What's a Theory</h2>
<a href="#TOC">Back to Table of Contents</a>

We start the conversation by introducing the concept of a [Theory](https://en.wikipedia.org/wiki/Theory), and discuss the influence of language (the choice of words we use/meaning we imply) when forming theories. The main points of emphasis are the following:

<ul>
    <li> Words have varying meaning depending on context; many events/ things are usually referred to by one word. Therefore, to develop theory (a general explanation/representation of phenomenon) the words we use/definitions we give tend to be narrow, and specific. We DO NOT consider all varying meanings of a singular word, or all contexts in which the word applies</li>
    <li>Theories are formed from an explanation of a collection of phenomenon (related portions of our experience).  They include assumptions we make, and the “mathematical working out of the logical consequences which must necessarily follow from those assumptions”. The consequences must match/agree with the complex phenomena if the theory is to be valid</li>
    <li>The assumptions and ideas of a theory determines its generality. The more general a theory is, the more broadly it can be applied, and the more useful/powerful it is</li>
</ul>

<h2 id="classification-theory">Classification of Theories</h2>
<a href="#TOC">Back to Table of Contents</a>

We then go onto describing the classification of theories. The scope/range of the theory as well as its place on the mathematical-physical spectrum determine its classification. The following are also things to keep in mind:
<ul>
    <li>Theories can be classified as being very narrow or very general in its scope. Simply stated, there is a limit to where theories can be applied</li>
    <li>Theories can also be classified as being strongly mathematical or strongly physical</li>
    <li>A theory is more physical if it “describes completely some range of physical phenomena which in practices is always limited” </li>
    <li>A theory is more mathematical if it “deals with an idealized class of phenomena (or certain aspects of phenomena” </li>
</ul>

<h2 id="shannon-overview">Brief Overview of Shannon's Theory of Communication</h2>
<a href="#TOC">Back to Table of Contents</a>

We also give a brief introduction of Shannon’s Information Theory (Also known as The Mathematical Theory of Communication). Specifically, we begin to describe the historical origins and details/properties of the theory.

<ul>
    <li>Information Theory grew out of the study of electrical communication. “However, it attacks problems in a very abstract and general way”; This includes the useful analysis of written/spoken language, electrical/mechanical transmission of messages, behavior of machines/maybe people</li>
    <li>The key unit of measure is the bit. It is a universal measure of the amount of information in terms of choice or uncertainty</li>
    <li>We also kind of allude to <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">Entropy (Information Theory)</a>. We also describe an objective of Information Theory as specifying or learning the choice between two equally probable alternatives (i.e: messages, or numbers to be transmitted)</li>
    <li>Encoding (represent) messages of a particular message source efficiently for transmission over a particular medium/channel is also discussed</li>
    <li>Finally, we talk about abstract properties of particular channels/mediums</li>
</ul>

We also describe the structure of Information Theory (and mathematical theory in general). They are a set of premises, axioms, and finally theorems (via proofs)

<ul>
    <li>“The truth of a theorem depends on the validity of the assumption, argument, and/or proof”</li>
    <li><a href="https://en.wikipedia.org/wiki/Mathematical_proof">Proofs</a> are rigorous (in that they’re complete, consistent, thorough, strict, and adhere to a certain standard)</li>
</ul>

<h2 id="Conclusion">Conclusion</h2>

We don't discuss much about Shannon's contributions in Chapter 1, we have the whole book for that, after all. Therefore, our summary of Chapter 1 ends on this note. Again, refer to <a href="#tldr">TL;DR</a> for an even more brief summary of what was covered in Chapter 1.

<img src="https://media.giphy.com/media/Y0btn5YtZRGNkTnvNx/giphy.gif">

<a href="#jump">Back to the Top</a>